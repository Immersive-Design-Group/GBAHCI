<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Presentation | Greater Bay HCI</title>
    <link>https://gbahci.com/prechi2025/tag/presentation/</link>
      <atom:link href="https://gbahci.com/prechi2025/tag/presentation/index.xml" rel="self" type="application/rss+xml" />
    <description>Presentation</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sat, 12 Apr 2025 15:30:00 +0800</lastBuildDate>
    <image>
      <url>https://gbahci.com/prechi2025/media/icon_hu_98056d0460235853.png</url>
      <title>Presentation</title>
      <link>https://gbahci.com/prechi2025/tag/presentation/</link>
    </image>
    
    <item>
      <title>Session A1 - GenAI-Enhanced Communication</title>
      <link>https://gbahci.com/prechi2025/program/session_a1/</link>
      <pubDate>Sat, 12 Apr 2025 10:10:00 +0800</pubDate>
      <guid>https://gbahci.com/prechi2025/program/session_a1/</guid>
      <description>&lt;hr /&gt;
&lt;h2 id=&#34;genai-enhanced-communication&#34;&gt;GenAI-Enhanced Communication&lt;/h2&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Session Host&lt;/strong&gt;â€‹: Zeyu Huang&lt;/p&gt;
&lt;h3 id=&#34;rambler-in-the-wild-a-diary-study-of-llm-assisted-writing-with-speech&#34;&gt;Rambler in the Wild: A Diary Study of LLM-Assisted Writing With Speech&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Xuyu Yang, &lt;em&gt;City University of Hong Kong&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Speech-to-text technologies have been shown to improve text input efficiency and potentially lower the barriers to writing. Recent LLM-assisted dictation tools aim to support writing with speech by bridging the gaps between speaking and traditional writing. This case study reports on the real-world writing experiences of twelve academic or creative writers using one such tool â€“ Rambler, to write various articles such as blog posts, diaries, screenplays, notes, or fictional stories, etc. Through a ten-day diary study, we identified the participants&amp;rsquo; in-context writing strategies using Rambler, such as how they expanded from an outline or organized their loose thoughts for different writing goals. The interviews uncovered the psychological and productivity affordances of writing with speech, pointing to future directions of designing for this writing modality and the utilization of AI support.&lt;/p&gt;
&lt;h3 id=&#34;scaffolded-turns-and-logical-conversations-designing-humanized-llm-powered-conversational-agents-for-hospital-admission-interviews&#34;&gt;Scaffolded Turns and Logical Conversations: Designing Humanized LLM-Powered Conversational Agents for Hospital Admission Interviews&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Dingdong Liu, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Hospital admission interviews are critical for patient care but strain nurses&amp;rsquo; capacity due to time constraints and staffing shortages.While LLM-powered conversational agents (CAs) offer automation potential, their rigid sequencing and lack of humanized communication skills risk misunderstandings and incomplete data capture.Through participatory design with clinicians and volunteers, we identified essential communication strategies and developed a novel CA that implements these strategies through: (1) dynamic topic management using graph-based conversation flows, and (2) context-aware scaffolding with few-shot prompt tuning.Technical evaluation on an admission interview dataset showed our system achieving performance comparable to or surpassing human-written ground truth, while outperforming prompt-engineered baselines.A between-subject study (N=44) demonstrated significantly improved user experience and data collection accuracy compared to existing solutions.We contribute a framework for humanizing medical CAs by translating clinician expertise into algorithmic strategies, alongside empirical insights for balancing efficiency and empathy in healthcare interactions, and considerations for generalizability.&lt;/p&gt;
&lt;h3 id=&#34;ronaldos-a-poser-how-the-use-of-generative-ai-shapes-debates-in-online-forums&#34;&gt;&amp;ldquo;Ronaldo&amp;rsquo;s a poser!&amp;rdquo;: How the Use of Generative AI Shapes Debates in Online Forums&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yuhan Zeng, &lt;em&gt;City University of Hong Kong&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Online debates can enhance critical thinking but may escalate into hostile attacks. As humans are increasingly reliant on Generative AI (GenAI) in writing tasks, we need to understand how people utilize GenAI in online debates. To examine the patterns of writing behavior while making arguments with GenAI, we created an online forum for soccer fans to engage in turn-based and free debates in a post format with the assistance of ChatGPT, arguing on the topic of &amp;ldquo;Messi vs Ronaldo&amp;rdquo;. After 13 sessions of two-part study and semi-structured interviews with 39 participants, we conducted content and thematic analyses to integrate insights from interview transcripts, ChatGPT records, and forum posts. We found that participants prompted ChatGPT for aggressive responses, created posts with similar content and logical fallacies, and sacrificed the use of ChatGPT for better human-human communication. This work uncovers how polarized forum members work with GenAI to engage in debates online.&lt;/p&gt;
&lt;h3 id=&#34;journalaide-empowering-older-adults-in-digital-journal-writing&#34;&gt;JournalAIde: Empowering Older Adults in Digital Journal Writing&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Shixu Zhou, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Digital journaling offers a means for older adults to express themselves, document their lives, and engage in self-reflection, contributing to the maintenance of cognitive function and social connectivity. Although previous works have investigated the motivations and benefits of digital journaling for older adults, little technical support has been designed to offer assistance. We conducted a formative study with older adults and uncovered their encountered challenges and preferences for technical support. Informed by the findings, we designed a Large Language Model (LLM) empowered tool, JournalAIde, which provides vicarious experience, idea organization, sample text generation, and visual editing cues to enhance older adultséˆ¥?confidence, writing ability, and sustained attention during digital journaling. Through a between-subjects study and a field deployment, we demonstrated the JournalAIdeéˆ¥æªš significant effectiveness compared to a baseline system in empowering older adults in digital journaling. We further investigated older adults&amp;rsquo; experiences and perceptions of LLM writing assistance.&lt;/p&gt;
&lt;h3 id=&#34;harmonycut-supporting-creative-chinese-paper-cutting-design-with-form-and-connotation-harmony&#34;&gt;HarmonyCut: Supporting Creative Chinese Paper-cutting Design with Form and Connotation Harmony&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Huanchen Wang, &lt;em&gt;Southern University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Chinese paper-cutting, an Intangible Cultural Heritage (ICH), faces challenges from the erosion of traditional culture due to the prevalence of realism alongside limited public access to cultural elements. While generative AI can enhance paper-cutting design with its extensive knowledge base and efficient production capabilities, it often struggles to align content with cultural meaning due to users&amp;rsquo; and models&amp;rsquo; lack of comprehensive paper-cutting knowledge. To address these issues, we conducted a formative study (N=7) to identify the workflow and design space, including four core factors (Function, Subject Matter, Style, and Method of Expression) and a key element (Pattern). We then developed HarmonyCut, a generative AI-based tool that translates abstract intentions into creative and structured ideas. This tool facilitates the exploration of suggested related content (knowledge, works, and patterns), enabling users to select, combine, and adjust elements for creative paper-cutting design. A user study (N=16) and an expert evaluation (N=3) demonstrated that HarmonyCut effectively provided relevant knowledge, aiding the ideation of diverse paper-cutting designs and maintaining design quality within the design space to ensure alignment between form and cultural connotation.&lt;/p&gt;
&lt;h3 id=&#34;acknowledge-a-computational-framework-for-human-compatible-affordance-based-interaction-planning-in-real-world-contexts&#34;&gt;ACKnowledge: A Computational Framework for Human Compatible Affordance-based Interaction Planning in Real-world Contexts&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Ziqi Pan, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Intelligent agents coexisting with humans often need to interact with human-shared objects in environments. Thus, agents should plan their interactions based on objects&amp;rsquo; affordances and the current situation to achieve acceptable outcomes. How to support intelligent agents&amp;rsquo; planning of affordance-based interactions compatible with human perception and values in real-world contexts remains under-explored. We conducted a formative study identifying the physical, intrapersonal, and interpersonal contexts that count to household human-agent interaction. We then proposed ACKnowledge, a computational framework integrating a dynamic knowledge graph, a large language model, and a vision language model for affordance-based interaction planning in dynamic human environments. In evaluations, ACKnowledge generated acceptable planning results with an understandable process. In real-world simulation tasks, ACKnowledge achieved a high execution success rate and overall acceptability, significantly enhancing usage-rights respectfulness and social appropriateness over baselines. The case study&amp;rsquo;s feedback demonstrated ACKnowledge&amp;rsquo;s negotiation and personalization capabilities toward an understandable planning process.&lt;/p&gt;
&lt;h3 id=&#34;exploring-the-design-of-llm-based-agent-in-enhancing-self-disclosure-among-the-older-adults&#34;&gt;Exploring the Design of LLM-based Agent in Enhancing Self-disclosure Among the Older Adults&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yijie Guo, &lt;em&gt;Tsinghua Univeristy&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Social difficulties have become an increasingly serious issue among older adults. For older adults, regular self-disclosure is essential for maintaining mental health and building close relationships. Leveraging conversational agents to encourage self-disclosure in older adults has shown increasing potential. Understanding how LLM-based agents can influence and stimulate self-disclosure across different topics is crucial for designing future agents tailored to older users. This study introduces Disclosure-Agent, an LLM-based conversational agent, and examines its impact on self-disclosure in older adults through a user study involving 20 participants, 8 topics, and two interactive interfaces equipped with Disclosure-Agent. The findings provide valuable insights into how LLM-based agents can promote self-disclosure in older adults and offer design recommendations for future elderly-oriented conversational agents.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Session B1 - Healthcare and Human Wellbeing</title>
      <link>https://gbahci.com/prechi2025/program/session_b1/</link>
      <pubDate>Sat, 12 Apr 2025 10:00:00 +0800</pubDate>
      <guid>https://gbahci.com/prechi2025/program/session_b1/</guid>
      <description>&lt;hr /&gt;
&lt;h2 id=&#34;healthcare-and-human-wellbeing&#34;&gt;Healthcare and Human Wellbeing&lt;/h2&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Session Host&lt;/strong&gt;â€‹: Liangwei Wang&lt;/p&gt;
&lt;h3 id=&#34;empathy-driven-interaction-design-guest-talk&#34;&gt;Empathy-Driven Interaction Design [Guest Talk]&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Wei Liu, &lt;em&gt;Southern University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Empathy is a fundamental principle in Human-Computer Interaction (HCI), shaping the design of intuitive, inclusive, and emotionally resonant experiences. This talk introduces Empathy-Driven Interaction Design (EDID), a human-centered framework that integrates psychological insights, interaction qualities, and experiential design to improve usability and accessibility. Applications in assistive technology, user-friendly interfaces, and emotion-aware systems will be explored, demonstrating how empathy can be systematically embedded in design processes. Case studies and research will highlight strategies for fostering deeper user engagement and creating meaningful interactions that go beyond functionality to establish genuine user connections.&lt;/p&gt;
&lt;h3 id=&#34;a-constructed-response-designing-and-choreographing-robot-arm-movements-in-collaborative-dance-improvisation&#34;&gt;A Constructed Response: Designing and Choreographing Robot Arm Movements in Collaborative Dance Improvisation&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Ray Lc, &lt;em&gt;City University of Hong Kong&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: In dance, dancers improvise and choreograph with each other, prototyping movement designs with each other. These interactions extend into collaboration with technology to enhance the creative process. We want to understand how dancers design and improvise movements together in the case of working with a robotic arm, which serves as an instrument in the stage space capable of non-humanoid movements. We engaged and observed dancers in a workshop to co-create movements with robots in one-human-to-one-robot and three-human-to-one-robot settings. We found that dancers produced more fluid movements in one-to-one scenarios, experiencing a stronger sense of connection and presence with the robot as a co-dancer. Conversely, in three-to-one scenarios, the dancers divided their attention between the human dancers and the robot, resulting in increased perceived use of space and more stop-and-go movements, perceiving the robot as part of the stage background. This work highlights how technologies can drive creativity in movement artists as they adapt to new ways of working with instruments, extending prior research on dancing with inanimate objects by exploring how robotic arms influence creative collaboration. We contribute insights into designing systems that support improvisational processes and artistic collaborations with non-humanoid agents.&lt;/p&gt;
&lt;h3 id=&#34;human-precision-medicine-interaction-public-perceptions-of-polygenic-risk-score-for-genetic-health-prediction&#34;&gt;Human-Precision Medicine Interaction: Public Perceptions of Polygenic Risk Score for Genetic Health Prediction&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yuhao Sun, &lt;em&gt;University of Edinburgh&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Precision Medicine (PM) transforms the traditional &amp;ldquo;one-drug-fits-all&amp;rdquo; paradigm by customising treatments based on individual characteristics, and is an emerging topic for HCI research on digital health. A key element of PM, the Polygenic Risk Score (PRS), uses genetic data to predict an individual&amp;rsquo;s disease risk. Despite its potential, PRS faces barriers to adoption, such as data inclusivity, psychological impact, and public trust. We conducted a mixed-methods study to explore how people perceive PRS, formed of surveys (n=254) and interviews (n=11) with UK-based participants. The interviews were supplemented by interactive storyboards with the ContraVision technique to provoke deeper reflection and discussion. We identified ten key barriers and five themes to PRS adoption and proposed design implications for a responsible PRS framework. To address the complexities of PRS and enhance broader PM practices, we introduce the term Human-Precision Medicine Interaction (HPMI), which integrates, adapts, and extends HCI approaches to better meet these challenges.&lt;/p&gt;
&lt;h3 id=&#34;designing-and-evaluating-a-narrative-driven-spatial-visualization-for-improving-patient-centered-communication-among-older-adults&#34;&gt;Designing and Evaluating a Narrative-driven Spatial Visualization for Improving Patient-centered Communication among Older Adults&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Jiaan Li, &lt;em&gt;Hong Kong Polytechnic University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: With the aging population, older adult patients experience difficulties in communicating and understanding medical information within healthcare settings. This late-breaking work focuses on designing and evaluating narrative-driven spatial visualization (N-dSV) through conducting iterative participatory design with clinicians and older adults. The results indicate that compared with the traditional paper-based mode, this innovative information communication mode can significantly improve older adultséˆ¥?understanding of medical information and have a higher willingness to use it, thus improving the patient-centered communication experience. Finally, we propose three design implications to provide references for research in the HCI field to optimize patient-centered communication in medical scenarios by using narration-driven spatial visualization as a framework.&lt;/p&gt;
&lt;h3 id=&#34;walk-in-their-shoes-to-navigate-your-own-path-learning-about-procrastination-through-a-serious-game&#34;&gt;Walk in Their Shoes to Navigate Your Own Path: Learning About Procrastination Through A Serious Game&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Runhua Zhang, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Procrastination, the voluntary delay of tasks despite potential negative consequences, has prompted numerous time and task management interventions in the HCI community. While these interventions have shown promise in addressing specific behaviors, psychological theories suggest that learning about procrastination itself may help individuals develop their own coping strategies and build mental resilience. However, little research has explored how to support this learning process through HCI approaches. We present ProcrastiMate, a text adventure game where players learn about procrastination&amp;rsquo;s causes and experiment with coping strategies by guiding in-game characters in managing relatable scenarios. Our field study with 27 participants revealed that ProcrastiMate facilitated learning and self-reflection while maintaining psychological distance, motivating players to integrate newly acquired knowledge in daily life. This paper contributes empirical insights on leveraging serious games to facilitate learning about procrastination and offers design implications for addressing psychological challenges through HCI approaches.&lt;/p&gt;
&lt;h3 id=&#34;customizing-emotional-support-how-do-individuals-construct-and-interact-with-llm-powered-chatbots&#34;&gt;Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Xi Zheng, &lt;em&gt;City University of Hong Kong&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Personalized support is essential to fulfill individuals&amp;rsquo; emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.&lt;/p&gt;
&lt;h3 id=&#34;becoming-my-own-audience-how-dancers-react-to-avatars-unlike-themselves-in-motion-capture-supported-live-improvisational-performance&#34;&gt;&amp;ldquo;Becoming My Own Audience&amp;rdquo;: How Dancers React to Avatars Unlike Themselves in Motion Capture-Supported Live Improvisational Performance&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Fan Zhang, &lt;em&gt;City University of Hong Kong&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: The use of motion capture in live dance performances has created an emerging discipline enabling dancers to play different avatars on the digital stage. Unlike classical workflows, avatars enable performers to act as different characters in customized narratives, but research has yet to address how movement, improvisation, and perception change when dancers act as avatars. We created five avatars representing differing genders, shapes, and body limitations, and invited 15 dancers to improvise with each in practice and performance settings. Results show that dancers used avatars to distance themselves from their own habitual movements, exploring new ways of moving through differing physical constraints. Dancers explored using gender-stereotyped movements like powerful or feminine actions, experimenting with gender identity. However, focusing on avatars can coincide with a lack of continuity in improvisation. This work shows how emerging practices with performance technology enable dancers to improvise with new constraints, stepping outside the classical stage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Session A2 - Interactive Systems and Data Visualization</title>
      <link>https://gbahci.com/prechi2025/program/session_a2/</link>
      <pubDate>Sat, 12 Apr 2025 13:30:00 +0800</pubDate>
      <guid>https://gbahci.com/prechi2025/program/session_a2/</guid>
      <description>&lt;hr /&gt;
&lt;h2 id=&#34;interactive-systems-and-data-visualization&#34;&gt;Interactive Systems and Data Visualization&lt;/h2&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Session Host&lt;/strong&gt;â€‹: Xi Zheng&lt;/p&gt;
&lt;h3 id=&#34;making-invisible-dynamics-visible-case-studies-in-visualizing-computational-and-behavioral-processes-guest-talk&#34;&gt;Making Invisible Dynamics Visible: Case Studies in Visualizing Computational and Behavioral Processes [Guest Talk]&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yuxin Ma, &lt;em&gt;Southern University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: TBD&lt;/p&gt;
&lt;h3 id=&#34;vizta-enhancing-comprehension-of-distributional-visualization-with-visual-lexical-fused-conversational-interface&#34;&gt;VIzTA: Enhancing Comprehension of Distributional Visualization with Visual-Lexical Fused Conversational Interface&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Liangwei Wang, &lt;em&gt;The Hong Kong University of Science and Technology (Guangzhou))&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Comprehending visualizations requires readers to interpret visual encoding and the underlying meanings actively. This poses challenges for visualization novices, particularly when interpreting distributional visualizations that depict statistical uncertainty. Advancements in LLM-based conversational interfaces show promise in promoting visualization comprehension. However, they fail to provide contextual explanations at fine-grained granularity, and chart readers are still required to mentally bridge visual information and textual explanations during conversations.Our formative study highlights the expectations for both lexical and visual feedback, as well as the importance of explicitly linking these two modalities throughout the conversation. The findings motivate the design of VIzTA, a visualization teaching assistant that leverages the fusion of visual and lexical feedback to help readers better comprehend visualization. VIzTA features a semantic-aware conversational agent capable of explaining contextual information within visualizations and employs a visual-lexical fusion design to facilitate chart-centered conversation. A between-subject study with 24 participants demonstrates the effectiveness of VIzTA in supporting the understanding and reasoning tasks of distributional visualization across multiple scenarios.&lt;/p&gt;
&lt;h3 id=&#34;insightbridge-enhancing-empathizing-with-users-through-real-time-information-synthesis-and-visual-communication&#34;&gt;InsightBridge: Enhancing Empathizing with Users through Real-Time Information Synthesis and Visual Communication&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yue Zhang, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: User-centered design necessitates researchers deeply understanding target users throughout the design process. However, during early-stage user interviews, researchers may misinterpret users due to time constraints, incorrect assumptions, and communication barriers. To address this challenge, we introduce InsightBridge, a tool that supports real-time, AI-assisted information synthesis and visual-based verification. InsightBridge automatically organizes relevant information from ongoing interview conversations into an empathy map. It further allows researchers to specify elements to generate visual abstracts depicting the selected information, and then review these visuals with users to refine the visuals as needed. We evaluated the effectiveness of InsightBridge through a within-subject study (N=32) from both the researchers&amp;rsquo; and users&amp;rsquo; perspectives. Our findings indicate that InsightBridge can assist researchers in note-taking and organization, as well as in-time visual checking, thereby enhancing mutual understanding with users. Additionally, users&amp;rsquo; discussions of visuals prompt them to recall overlooked details and scenarios, leading to more insightful ideas.&lt;/p&gt;
&lt;h3 id=&#34;interlink-linking-text-with-code-and-outputs-in-computational-notebooks&#34;&gt;InterLink: Linking Text with Code and Outputs in Computational Notebooks&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yanna Lin, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Computational notebooks, widely used for ad-hoc analysis and often shared with others, can be difficult to understand because the standard linear layout is not optimized for reading. In particular, related text, code, and outputs may be spread across the UI making it difficult to draw connections. In response, we introduce InterLink, a plugin designed to present the relationships between text, code, and outputs, thereby making notebooks easier to understand. In a formative study, we identify pain points and derive design requirements for identifying and navigating relationships among various pieces of information within notebooks. Based on these requirements, InterLink features a new layout that separates text from code and outputs into two columns. It uses visual links to signal relationships between text and associated code and outputs and offers interactions for navigating related pieces of information. In a user study with 12 participants, those using InterLink were 13.6% more accurate at finding and integrating information from complex analyses in computational notebooks. These results show the potential of notebook layouts that make them easier to understand.&lt;/p&gt;
&lt;h3 id=&#34;creative-blends-of-visual-concepts&#34;&gt;Creative Blends of Visual Concepts&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yaozhen Zhang, &lt;em&gt;Shenzhen University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Visual blends combine elements from two distinct visual concepts into a single, integrated image, with the goal of conveying ideas through imaginative and often thought-provoking visuals. Communicating abstract concepts through visual blends poses a series of conceptual and technical challenges. To address these challenges, we introduce Creative Blends, an AI-assisted design system that leverages metaphors to visually symbolize abstract concepts by blending disparate objects. Our method harnesses commonsense knowledge bases and large language models to align designers&amp;rsquo; conceptual intent with expressive concrete objects. Additionally, we employ generative text-to-image techniques to blend visual elements through their overlapping attributes. A user study (N=24) demonstrated that our approach reduces participants&amp;rsquo; cognitive load, fosters creativity, and enhances the metaphorical richness of visual blend ideation. We explore the potential of our method to expand visual blends to include multiple object blending and discuss the insights gained from designing with generative AI.&lt;/p&gt;
&lt;h3 id=&#34;gencolor-generative-color-concept-association-in-visual-design&#34;&gt;GenColor: Generative Color-Concept Association in Visual Design&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yihan Hou, &lt;em&gt;The Hong Kong University of Science and Technology (Guangzhou)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Existing approaches for color-concept association typically rely on query-based image referencing, and color extraction from image references. However, these approaches are effective only for common concepts, and are vulnerable to unstable image referencing and varying image conditions. Our formative study with designers underscores the need for primary-accent color compositions and context-dependent colors (e.g., &amp;lsquo;clear&amp;rsquo; vs. &amp;lsquo;polluted&amp;rsquo; sky) in design. In response, we introduce a generative approach for mining semantically resonant colors leveraging images generated by text-to-image models. Our insight is that contemporary text-to-image models can resemble visual patterns from large-scale real-world data. The framework comprises three stages: concept instancing produces generative samples using diffusion models, text-guided image segmentation identifies concept-relevant regions within the image, and color association extracts primarily accompanied by accent colors. Quantitative comparisons with expert designs validate our approach&amp;rsquo;s effectiveness, and we demonstrate the applicability through cases in various design scenarios and a gallery.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Session B2 - New Media and Research Inspirations</title>
      <link>https://gbahci.com/prechi2025/program/session_b2/</link>
      <pubDate>Sat, 12 Apr 2025 13:30:00 +0800</pubDate>
      <guid>https://gbahci.com/prechi2025/program/session_b2/</guid>
      <description>&lt;hr /&gt;
&lt;h2 id=&#34;new-media-and-research-inspirations&#34;&gt;New Media and Research Inspirations&lt;/h2&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Session Host&lt;/strong&gt;â€‹: Hanfang Lyu&lt;/p&gt;
&lt;h3 id=&#34;design-futures-in-hci-design-thinking-for-digital-transformation--guest-talk&#34;&gt;Design Futures in HCI: Design Thinking for Digital Transformation  [Guest Talk]&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Zhiyong Fu, &lt;em&gt;Tsinghua University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: TBD&lt;/p&gt;
&lt;h3 id=&#34;participatory-design-in-human-computer-interaction-cases-characteristics-and-lessons&#34;&gt;Participatory Design in Human-Computer Interaction: Cases, Characteristics, and Lessons&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Xiang (nathan) Qi, &lt;em&gt;The Hong Kong Polytechnic University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Participatory Design (PD) has become increasingly prevalent inHuman-Computer Interaction (HCI) research. However, there remains a lack of comprehensive understanding of how PD has beenused by HCI scholars. To bridge this gap, we sampled PD applicationcases (ğ‘ = 185) from the SIGCHI conferences over the past decadeand examined these cases through the dimensions of applicationfeatures (e.g., contexts and functions of PD) and PD principles (e.g.,its political commitment and mutual learning principle). Our analysis reveals the various ways PD has been applied in HCI and howits core features have been or have not been manifested in thesecases. Based on these findings, we reflect on the conceptual understanding of PD within the HCI community and discuss potentialmisconceptions. Ultimately, we hope this work can serve as a usefulreference for HCI researchers and beyond who are interested inincorporating PD into their design and research practices.&lt;/p&gt;
&lt;h3 id=&#34;seqr-a-user-friendly-and-secure-by-design-configurator-for-enterprise-wi-fi&#34;&gt;SeQR: A User-Friendly and Secure-by-Design Configurator for Enterprise Wi-Fi&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Sze Yiu Chau, &lt;em&gt;The Chinese University of Hong Kong&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: A classic problem in enterprise Wi-Fi is client-side misconfiguration, which enables credential theft via éˆ¥æ·“vil Twinéˆ¥?(ET) attacks. To mitigate this, we design, develop, and evaluate a new configurator, SeQR, which allows users to effortlessly and securely set up an enterprise Wi-Fi connection. Utilizing existing authenticated channels, SeQR fully automates the client-side enterprise Wi-Fi configuration process with a simple scan, leaving no room for misconfigurations. Specifically, SeQR thwarts ET by making it impossible for users to opt-out from the security-critical certificate validation. We evaluate the efficacy of SeQR on two fronts. First, we implement a prototype of SeQR in Android, and test its functionality and runtime performance. Next, we compare the usability of SeQR against two existing Wi-Fi configuration interfaces of Android in an in-person user study (n=41) with real devices. Our evaluation shows that SeQR achieves noticeable usability improvements over existing designs, and prevents users from misconfiguring.&lt;/p&gt;
&lt;h3 id=&#34;signaling-human-intentions-to-service-robots-understanding-the-use-of-social-cues-during-in-person-conversations&#34;&gt;Signaling Human Intentions to Service Robots: Understanding the Use of Social Cues during In-Person Conversations&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Hanfang Lyu, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: As service robots become commonplace, it is essential for them to effectively interpret human signals, such as verbal, gesture, and eye gaze, in social contexts, where people need to focus on their primary tasks to minimize interruptions and distractions. Towards such a socially acceptable Human-Robot Interaction, we conducted a study (N=24) in an AR-simulated context of a coffee chat. Participants elicited social cues for signaling intentions to an anthropomorphic, zoomorphic, grounded technical, or aerial technical robot waiter when they were speakers or listeners. Our findings reveal the common patterns of social cues over intentions, the effects of robot morphology on social cue position and conversational role on social cue complexity, and users&amp;rsquo; rationale in choosing social cues. We offer insights into understanding social cues concerning perceptions of robots, cognitive load, and social context. Additionally, we discuss design considerations on approaching, social cue recognition, and response strategies for future service robots.&lt;/p&gt;
&lt;h3 id=&#34;exploring-the-evolvement-of-user-engagement-in-online-creative-community-under-the-surge-of-generative-ai-a-case-study-of-deviantart&#34;&gt;Exploring the Evolvement of User Engagement in Online Creative Community under the Surge of Generative AI: A Case Study of DeviantArt&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Kangyu Yuan, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: The rise of AI-generated content (AIGC) is transforming online creative communities (OCCs) and posing challenges to their regulation. The interacting behaviors, such as sharing artworks with descriptions, commenting on creations, and creators&amp;rsquo; subsequent replying are the essential components of user engagement in these communities. Understanding the influence of AIGC on the evolving user engagement could be helpful for community regulation. In this work, we collect 235K posts and their associated 255K comments from DeviantArt, a large creative community allowing uploading AIGC. Through open coding, we identify five categories of practices in describing and commenting on artworks, respectively. A set of deep learning models are applied to classify the posts and comments. We then combine time series regression analysis, causal inference analysis, and logistic regression analysis, to examine the impact of the surge of AIGC on user engagement. Results suggest that AI-generated artworks show a decreasing emphasis on the content of creations but an increasing trend toward commercial and promotion purposes. AI-generated artworks emphasize less on IP issues than human-created ones, while the awareness of IP issues drops for human-created artworks with the growth of AIGC as well. Although comments with high sentiment valence, for peer bonding or for requesting usage positively predict the reply behavior for human-created artworks, community members are less likely to maintain these interactions as AIGC rises. Finally, we discuss insights and design implications for OCCs.&lt;/p&gt;
&lt;h3 id=&#34;designing-highly-accessible-xr-interfaces-rep&#34;&gt;Designing Highly Accessible XR Interfaces [REP]&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yang Tian, &lt;em&gt;Guangxi University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: TBD&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Session A3 - Interactive Systems and Data Visualization</title>
      <link>https://gbahci.com/prechi2025/program/session_a3/</link>
      <pubDate>Sat, 12 Apr 2025 15:30:00 +0800</pubDate>
      <guid>https://gbahci.com/prechi2025/program/session_a3/</guid>
      <description>&lt;hr /&gt;
&lt;h2 id=&#34;technology-enhanced-learning-and-heritage&#34;&gt;Technology-Enhanced Learning and Heritage&lt;/h2&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Session Host&lt;/strong&gt;â€‹: Yu&amp;rsquo;an Chen&lt;/p&gt;
&lt;h3 id=&#34;culture-inheritance-and-design-innovation-research-practice-at-milab-guest-talk&#34;&gt;Culture Inheritance and Design Innovation: Research Practice at MILAB [Guest Talk]&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Haipeng Mi, &lt;em&gt;Tsinghua University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: TBD&lt;/p&gt;
&lt;h3 id=&#34;coknowledge-supporting-assimilation-of-time-synced-collective-knowledge-in-online-science-videos&#34;&gt;CoKnowledge: Supporting Assimilation of Time-synced Collective Knowledge in Online Science Videos&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yuanhao Zhang, &lt;em&gt;The Hong Kong University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Danmaku, a system of scene-aligned, time-synced, floating comments, can augment video content to create `collective knowledge&amp;rsquo;. However, its chaotic nature often hinders viewers from effectively assimilating the collective knowledge, especially in knowledge-intensive science videos. With a formative study, we examined viewers&amp;rsquo; practices for processing collective knowledge and the specific barriers they encountered. Building on these insights, we designed a processing pipeline to filter, classify, and cluster danmaku, leading to the development of CoKnowledge &amp;ndash; a tool incorporating a video abstract, knowledge graphs, and supplementary danmaku features to support viewers&amp;rsquo; assimilation of collective knowledge in science videos. A within-subject study (N=24) showed that CoKnowledge significantly enhanced participants&amp;rsquo; comprehension and recall of collective knowledge compared to a baseline with unprocessed live comments. Based on our analysis of user interaction patterns and feedback on design features, we presented design considerations for developing similar support tools.&lt;/p&gt;
&lt;h3 id=&#34;designing-llm-powered-multimodal-instructions-to-support-rich-hands-on-skills-remote-learning-a-case-study-with-massage-instructors-and-learners&#34;&gt;Designing LLM-Powered Multimodal Instructions to Support Rich Hands-on Skills Remote Learning: A Case Study with Massage Instructors and Learners&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Chutian Jiang, &lt;em&gt;The Hong Kong University of Science and Technology (Guangzhou)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Although remote learning is widely used for delivering and capturing knowledge, it has limitations in teaching hands-on skills that require nuanced instructions and demonstrations of precise actions, such as massage. Furthermore, scheduling conflicts between instructors and learners often limit the availability of real-time feedback, reducing learning efficiency. To address these challenges, we developed a synthesis tool utilizing an LLM-powered Virtual Teaching Assistant (VTA). This tool integrates multimodal instructions that convey precise data, such as stroke patterns and pressure control, while providing real-time feedback for learners and summarizing their performance for instructors. Our case study with instructors and learners demonstrated the effectiveness of these multimodal instructions and the VTA in enhancing massage teaching and learning. We then discuss the tools&amp;rsquo; use in other hands-on skills instruction and cognitive process differences in various courses.&lt;/p&gt;
&lt;h3 id=&#34;breaking-barriers-or-building-dependency-exploring-team-llm-collaboration-in-ai-infused-classroom-debate&#34;&gt;Breaking Barriers or Building Dependency? Exploring Team-LLM Collaboration in AI-infused Classroom Debate&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Zihan Zhang, &lt;em&gt;Southern University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Breaking Barriers or Building Dependency? Exploring Team-LLM Collaboration in AI-infused Classroom Debate of debates, the role of AI tools, particularly LLM-based systems, in supporting this dynamic learning environment has been under- explored in HCI. This study addresses this opportunity by investi- gating the integration of LLM-based AI into real-time classroom debates. Over four weeks, 22 students in a Design History course participated in three rounds of debates with support from Chat- GPT. The findings reveal how learners prompted the AI to offer insights, collaboratively processed its outputs, and divided labor in team-AI interactions. The study also surfaces key advantages of AI usageâ€”reducing social anxiety, breaking communication barriers, and providing scaffolding for novicesâ€”alongside risks, such as in- formation overload and cognitive dependency, which could limit learners&amp;rsquo; autonomy. We thereby discuss a set of nuanced implica- tions for future HCI exploration.&lt;/p&gt;
&lt;h3 id=&#34;litlinker-supporting-the-ideation-of-interdisciplinary-contexts-with-large-language-models-for-teaching-literature-in-elementary-schools&#34;&gt;LitLinker: Supporting the Ideation of Interdisciplinary Contexts with Large Language Models for Teaching Literature in Elementary Schools&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Haoxiang Fan, &lt;em&gt;Sun Yat-sen University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Teaching literature under interdisciplinary contexts (e.g., science, art) that connect reading materials has become popular in elementary schools. However, constructing such contexts is challenging as it requires teachers to explore substantial amounts of interdisciplinary content and link it to the reading materials. In this paper, we develop LitLinker via an iterative design process involving 13 teachers to facilitate the ideation of interdisciplinary contexts for teaching literature. Powered by a large language model (LLM), LitLinker can recommend interdisciplinary topics and contextualize them with the literary elements (e.g., paragraphs, viewpoints) in the reading materials. A within-subjects study (N=16) shows that compared to an LLM chatbot, LitLinker can improve the integration depth of different subjects and reduce workload in this ideation task. Expert interviews (N=9) also demonstrate LitLinker&amp;rsquo;s usefulness for supporting the ideation of interdisciplinary contexts for teaching literature. We conclude with concerns and design considerations for supporting interdisciplinary teaching with LLMs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Session B3 - Interaction in VR/AR</title>
      <link>https://gbahci.com/prechi2025/program/session_b3/</link>
      <pubDate>Sat, 12 Apr 2025 15:30:00 +0800</pubDate>
      <guid>https://gbahci.com/prechi2025/program/session_b3/</guid>
      <description>&lt;hr /&gt;
&lt;h2 id=&#34;interaction-in-vrar&#34;&gt;Interaction in VR/AR&lt;/h2&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Session Host&lt;/strong&gt;â€‹: Yuan Xu&lt;/p&gt;
&lt;h3 id=&#34;gazepuffer--hands-free-input-method-leveraging-puff-cheeks-for-vr-guest-talk&#34;&gt;GazePuffer : Hands-Free Input Method Leveraging Puff Cheeks for VR [Guest Talk]&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Minghui Sun, &lt;em&gt;Jilin University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: TBD&lt;/p&gt;
&lt;h3 id=&#34;aiget-transforming-everyday-moments-into-hidden-knowledge-discovery-with-ai-assistance-on-smart-glasses&#34;&gt;AiGet: Transforming Everyday Moments into Hidden Knowledge Discovery with AI Assistance on Smart Glasses&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Runze Cai, &lt;em&gt;National University of Singapore&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Unlike the free exploration of childhood, the demands of daily life reduce our motivation to explore our surroundings, leading to missed opportunities for informal learning. Traditional tools for knowledge acquisition are reactive, relying on user initiative and limiting their ability to uncover hidden interests. Through formative studies, we introduce AiGet, a proactive AI assistant integrated with AR smart glasses, designed to seamlessly embed informal learning into low-demand daily activities (e.g., casual walking and shopping). AiGet analyzes real-time user gaze patterns, environmental context, and user profiles, leveraging large language models to deliver personalized, context-aware knowledge with low disruption to primary tasks. In-lab evaluations and real-world testing, including continued use over multiple days, demonstrate AiGetâ€™s effectiveness in uncovering overlooked yet surprising interests, enhancing primary task enjoyment, reviving curiosity, and deepening connections with the environment. We further propose design guidelines for AI-assisted informal learning, focused on transforming everyday moments into enriching learning experiences.&lt;/p&gt;
&lt;h3 id=&#34;augmenting-realistic-charts-with-virtual-overlays&#34;&gt;Augmenting Realistic Charts with Virtual Overlays&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Yao Shi, &lt;em&gt;The Hong Kong University of Science and Technology (Guangzhou)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: In this paper, we introduce the concept of realistic charts, referring to charts in the real world that cannot be digitally altered, such as those printed in newspapers or used in presentations. By enabling interaction with and graphical enhancement of these realistic charts as if they were digital, we transform realistic charts into â€œdigital chartsâ€ by adding virtual graphical overlays. To achieve this, we identify 33 overlay strategies (e.g., highlights and trendlines) for five widely-used chart types (e.g., line charts) through systematic exploration and a formative study. To simplify overlay creation, we introduce a new grammar named Vega-Overlay. Leveraging this design space and grammar, we develop a system called HARVis, which allows users to generate virtual overlays through augmented reality devices using speech and optional gestures. A user study involving 33 participants from diverse fields, across 17 tasks, demonstrates the effectiveness and usability of HARVis.&lt;/p&gt;
&lt;h3 id=&#34;modeling-locomotion-with-body-angular-movements-in-virtual-reality&#34;&gt;Modeling Locomotion with Body Angular Movements in Virtual Reality&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Zijun Mai, &lt;em&gt;Jinan University&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: This study proposes a time prediction model for locomotion along a polyline path with body angular movements in Virtual Reality (VR). We divide such locomotion into two components: navigating in multiple line-segment paths and turning at line-segment intersections. In the first component, locomotion in each line-segment path consists of acceleration, maximum velocity, and deceleration phases. We formulate equations to estimate the locomotion time for each phase and accumulated them to model the total time. In the second component, a linear relationship was revealed between task time and turning angles. We established an integrated model based on the equations of the two components and verified the effectiveness of the model with three experiments. The results indicate that our model outperformed two baseline models with a greater Î¾RË†2Î¾ and a smaller gap between the predicted and actual time. Our study benefits VR locomotion design with body angular movements.&lt;/p&gt;
&lt;h3 id=&#34;vrcaptions-design-captions-for-dhh-users-in-multiplayer-communication-in-vr&#34;&gt;VRCaptions: Design Captions for DHH Users in Multiplayer Communication in VR&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Tianze Xie, &lt;em&gt;Southern University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: Accessing auditory information remains challenging for DHH individuals in real-world situations and multiplayer VR interactions. To improve this, we investigated caption designs that specialize in the needs of DHH users in multiplayer VR settings. First, we conducted three co-design workshops with DHH participants, social workers, and designers to gather insights into the specific needs of design directions for DHH users in the context of a room escape game in VR. We further refined our designs with 13 DHH users to determine the most preferred features. Based on this, we developed VRCaptions, a caption prototype for DHH users to better experience multiplayer conversations in VR. We lastly invited two mixed-hearing groups to participate in the VR room escape game with our VRCaptions to validate. The results demonstrate that VRCaptions can enhance the ability of DHH participants to access information and reduce the barrier to communication in VR.&lt;/p&gt;
&lt;h3 id=&#34;reachpad-interacting-with-multiple-virtual-screens-using-a-single-physical-pad-through-haptic-retargeting&#34;&gt;ReachPad: Interacting with Multiple Virtual Screens using a Single Physical Pad through Haptic Retargeting&lt;/h3&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Speakerâ€‹&lt;/strong&gt;â€‹: Han Shi, &lt;em&gt;Southern University of Science and Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;strong&gt;â€‹Abstractâ€‹&lt;/strong&gt;â€‹: The advancement of Virtual Reality (VR) has expanded 2D userinterfaces into 3D space. This change has introduced richer interaction modalities but also brought challenges, especially the lackof haptic feedback in mid-air interactions. Previous research hasexplored various methods to provide feedback for interface interactions, but most approaches require specialized haptic devices.We introduce haptic retargeting to enable users to control multiple virtual screens in VR using a simple flat pad, which servesas a single physical proxy to support seamless interaction acrossmultiple virtual screens. We conducted user studies to explore theappropriate virtual screen size and positioning under our retargeting method and then compared various drag-and-drop methods for cross-screen interaction. Finally, we compared our method withcontroller-based interaction in application scenarios.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
